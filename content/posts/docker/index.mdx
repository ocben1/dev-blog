---
title: Docker & the it-works-on-my-machine Problem
date: 2022-02-19
tags:
 - docker
 - kubernetes
 - ci/cd
 - continuous integration
 - continuous deployment
 - containers
 - devops
---
Before Docker's rise to prominence, deploying applications was often a cumbersome and error-prone process. Developers faced numerous challenges when trying to ensure that their applications ran consistently across different environments, from development laptops to production servers. These challenges included dealing with differences in system configurations, dependencies, and runtime environments, making it difficult to reproduce and debug issues.

Docker's genius lies in its containerization technology, which encapsulates an application and its dependencies into a lightweight, isolated container. Here are some key reasons why Docker has had such a profound impact:
#### It works on my machine???/
Docker containers ensure that applications run **consistently** across various environments, eliminating the infamous "It works on my machine" problem. Developers can package everything an application needs, from libraries to configuration files, into a single container.
Docker containers are **highly portable**, which means that you can develop an application on your laptop, test it in a staging environment, and then deploy it to productionâ€”all with the same container image. This portability simplifies the deployment process and reduces the chances of errors caused by environment discrepancies.
Docker containers are incredibly lightweight and start quickly. This **efficiency** makes it possible to deploy and scale applications more efficiently, reducing infrastructure costs and providing a smoother user experience.
Containers **isolate** applications and their dependencies, ensuring that they do not interfere with each other. This isolation enhances security and makes it easier to manage and update applications.

### Docker's Impact on DevOps

The rise of Docker coincided with the DevOps movement, which emphasizes collaboration between development and operations teams. Docker's containerization technology fits seamlessly into the DevOps philosophy by enabling continuous integration, continuous delivery (CI/CD), and automated testing.

With Docker, organizations can create consistent build and deployment pipelines, resulting in faster release cycles and improved software quality. DevOps teams can now work together more effectively to deliver value to customers at a rapid pace.

### Use case: Database Management System
#### Problem
Imagine you're a developer working on a web app that uses a specific version of a database management system (DBMS) and a unique set of dependencies. It works perfectly on your development machine, but when you deploy it to the production server, it breaks due to differences in the environment.

#### Solution

With Docker, you can create a containerized environment that includes your application, the required version of the DBMS, and all the dependencies. This ensures that your application runs consistently across different environments, eliminating the "It works on my machine" problem.

#### Example
Let's spin up a simple Flask web application and a PostgreSQL database:

1. **Dockerfile for Flask Web App:**

   Create a Dockerfile in your project directory to define your application's environment:

   ```Dockerfile
   # Use an official Python runtime as a parent image
   FROM python:3.8-slim
   
   # Set the working directory to /app
   WORKDIR /app
   
   # Copy the current directory contents into the container at /app
   COPY . /app
   
   # Install any needed packages specified in requirements.txt
   RUN pip install -r requirements.txt
   
   # Make port 80 available to the world outside this container
   EXPOSE 80
   
   # Define environment variable
   ENV NAME World
   
   # Run app.py when the container launches
   CMD ["python", "app.py"]
   ```

2. **Docker Compose for PostgreSQL Database:**

   Use Docker Compose to define and run your database container alongside your web app container:

   ```yaml
   version: '3'
   services:
     web:
       build: .
       ports:
         - "80:80"
       depends_on:
         - db
     db:
       image: postgres:12
       environment:
         POSTGRES_DB: mydatabase
         POSTGRES_USER: myuser
         POSTGRES_PASSWORD: mypassword
   ```

3. **Application Code:**

   Develop your Flask web application (`app.py`) and specify dependencies in a `requirements.txt` file.

   ```python
   from flask import Flask
   import psycopg2
   
   app = Flask(__name__)
   
   @app.route('/')
   def hello():
       conn = psycopg2.connect(
           dbname='mydatabase',
           user='myuser',
           password='mypassword',
           host='db'
       )
       return 'Hello, World!'
   
   if __name__ == '__main__':
       app.run(host='0.0.0.0')
   ```

4. **Build and Run:**

   Now, you can build and run your Docker containers:

   ```bash
   docker-compose up --build
   ```

   Docker Compose will create two containers: one for your Flask web app and another for the PostgreSQL database. Your app will be accessible at `http://localhost`.

Using Docker, we've solved the real-life problem of environment consistency. Whether you run your application on your development machine or a production server, the environment inside the Docker containers remains the same, ensuring that your app behaves consistently, and you can avoid the frustrating issues caused by environment discrepancies.